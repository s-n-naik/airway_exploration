{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea069f35-53c3-44ef-aa08-716ff1eb7709",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "# !apt-get install -y xvfb\n",
    "import time\n",
    "import torch\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader, ImbalancedSampler\n",
    "from torch_geometric.data import Dataset\n",
    "# https://www.youtube.com/watch?v=QLIkOtKS4os --> creating custom dataset in pytorch geometric\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "import torch_geometric\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, GlobalAttention, SAGEConv\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sn\n",
    "import random\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "from graph_utils import set_device_and_seed, set_seed, show, visualize_graph, visualize_embedding, _count_parameters, visualise_airway_tree_matplotlib\n",
    "from graph_datasets import CustomDataset\n",
    "from graph_models import CustomGlobalAttention, GAT\n",
    "from graph_training import train_model, test_model, _vis_graph_example, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f511f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e86c8-5063-436e-b3a5-05c2c58368c4",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df9e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.read_csv(os.path.abspath('/home/sneha/toy_normalised_0607.csv'))\n",
    "orig_df.describe()\n",
    "\n",
    "label_df = pd.read_csv(os.path.abspath('/home/sneha/toy_labels_0607.csv'))\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete a lobe\n",
    "# orig_df = orig_df.loc[((orig_df.binaryLL_1 == 1) & (orig_df.lobe != 2)) | (orig_df.binaryLL_1 == 0)]\n",
    "# test.loc[test.binaryLL_1 == 1].groupby('lobe').agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c0254-4846-4ec8-bac2-9be2e8752167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_df = orig_df[['idno', 'binaryLL_1']].drop_duplicates()\n",
    "# label_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab51e9",
   "metadata": {},
   "source": [
    "### Train test split mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65494c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col_name = 'node_label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc6b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = train_test_split(label_df, n_splits_test = 5, label_col_name= label_col_name, seed=0)\n",
    "\n",
    "pilot_df_train = orig_df.loc[orig_df.idno.isin(train_ids)]\n",
    "pilot_df_test = orig_df.loc[orig_df.idno.isin(test_ids)]\n",
    "binary_label_df_train = label_df.loc[label_df.idno.isin(train_ids)]\n",
    "binary_label_df_test = label_df.loc[label_df.idno.isin(test_ids)]\n",
    "print(\"Overall Label frequency distribution\", [(x, binary_label_df_test[label_col_name].tolist().count(x)) for x in set(binary_label_df_test[label_col_name].tolist())])\n",
    "\n",
    "\n",
    "# SAVE PILOT DF for training\n",
    "pilot_df_train.to_csv('/home/sneha/toy_lobe_cleaned_normalised_w_labels_train.csv')\n",
    "binary_label_df_train.to_csv('/home/sneha/toy_lobe_binary_labels_train.csv')\n",
    "pilot_df_test.to_csv('/home/sneha/toy_lobe_cleaned_normalised_w_labels_test.csv')\n",
    "binary_label_df_test.to_csv('/home/sneha/toy_lobe_binary_labels_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d1401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _vis_graph_example_2(dataloader,model, index, pilot_df_w_labels, device,visualise_g = False, save_path = None):\n",
    "    data = dataloader.dataset[index]\n",
    "    print(data)\n",
    "    label = data.y\n",
    "    g = to_networkx(data)\n",
    "    # get relabelmap\n",
    "    idno, relabel = dataloader.dataset.node_map[index]\n",
    "    # map to original df\n",
    "    vis_graph  =pilot_df_w_labels.loc[pilot_df_w_labels.idno==idno]\n",
    "    print(f\"This graph is for id: {idno}\")\n",
    "    display(vis_graph.head())\n",
    "    # get the relabelling to match the pytorch graph \n",
    "    vis_graph['start_node'] = vis_graph.startbpid.apply(lambda x:relabel[x])\n",
    "    vis_graph['end_node'] = vis_graph.endbpid.apply(lambda x:relabel[x])\n",
    "#     display(vis_graph[['startbpid', 'endbpid']+node_features + ['parent_loc_x_norm','parent_loc_y_norm']])\n",
    "#     print(data.x)\n",
    "    print(\"Getting model preds per node (node model needs to be a per node one)\")\n",
    "    model.eval()\n",
    "    x, weight = model(data.to(device))\n",
    "    print(\"weight\", len(weight[0]))\n",
    "    x = x.cpu().detach().numpy() # take off cuda\n",
    "    \n",
    "    weight = weight[0].cpu().detach().numpy() # take first item take off cuda\n",
    "    \n",
    "#     print(\"x\", x.shape[0], 'weight', weight.shape[0], 'label', label.shape[0])\n",
    "    \n",
    "    if model.agg == 'none':\n",
    "#         print('useing node preds')\n",
    "        # per node rather than per graph\n",
    "        preds = x.copy().squeeze()\n",
    "    else:\n",
    "        # per graph output - use per node (weights )for color\n",
    "#         print(\"using weights\")\n",
    "        preds = weight.copy().squeeze()\n",
    "#     print(\"preds shape\", preds.shape)\n",
    "    \n",
    "    \n",
    "    # drawing graph in networkx & matplotlib\n",
    "    cmap = mpl.colormaps['spring'].reversed()\n",
    "    vmin, vmax = min(weight), max(weight)\n",
    "    norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "    sm.set_array([])\n",
    "    cmap_nodes = [cmap(norm(preds[node])) for node in g.nodes()]\n",
    "    edge_labels = {edge:vis_graph.loc[vis_graph.end_node == edge[1]]['anatomicalname'].item() for edge in g.edges()}\n",
    "    pos = nx.planar_layout(g, scale=1, center=(0,0), dim=2)\n",
    "    \n",
    "    \n",
    "    if visualise_g:\n",
    "        f, ax = plt.subplots(figsize=(10,10))\n",
    "        nx.draw(g,pos=pos, with_labels=False,node_color=cmap_nodes, ax=ax)\n",
    "        nx.draw_networkx_edge_labels(g, pos,\n",
    "                                  edge_labels,\n",
    "                                     font_color='k',\n",
    "                                     font_size='10',\n",
    "                                  label_pos=0.5,\n",
    "\n",
    "                                    )\n",
    "        \n",
    "        cbar = plt.colorbar(sm)\n",
    "\n",
    "        plt.title(f'TRAINING GRAPH: {idno}, ANOMALY LABEL: {label.item()}')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    for i in range(len(vis_graph)):\n",
    "        row = vis_graph.iloc[i]\n",
    "        ax.plot([row.parent_loc_x,row.x], [row.parent_loc_y,row.y], [row.parent_loc_z, row.z], linestyle='-',linewidth=1, color= cmap(norm(preds[row.end_node])), label=row.end_node)\n",
    "        ax.scatter(row.x, row.y, row.z, marker='o',color= cmap(norm(preds[row.end_node])))\n",
    "    ax.grid(False)\n",
    "    ax.set_facecolor(color=(1,1,1))\n",
    "    cbar = plt.colorbar(sm)\n",
    "    plt.title(f'TRAINING GRAPH: {idno}, ANOMALY LABEL: {label.item()}')\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c262c73",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcce96c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 5\n",
    "try:\n",
    "    os.makedirs(f'/home/sneha/airway_exploration/graph_networks/run_{run_id}/')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "verbose= True\n",
    "# device = set_device_and_seed(GPU=True, gpu_name='cuda:0')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "count_epochs = 0\n",
    "test_every = 5\n",
    "weighted_loss = False\n",
    "model_agg = 'mean'\n",
    "dropout = 0\n",
    "hidden_dim1 = 8\n",
    "hidden_dim2 = 8\n",
    "lr=0.01\n",
    "weight_decay=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col_name='graph_label'\n",
    "\n",
    "#  removed from features for toy example, added edge features to node ones too just in case\n",
    "node_features = ['x_norm', 'y_norm', 'z_norm', 'dircosx_norm',\n",
    "               'dircosy_norm', 'dircosz_norm','angle_norm', 'weibel_generation_norm','dist_nn_in_lobe_norm',\n",
    "                 'num_desc_norm','max_path_length_norm','centerlinelength_norm','avginnerarea_norm',\n",
    "                'lobe_norm','sublobe_norm']\n",
    "edge_feature_names = ['centerlinelength_norm','avginnerarea_norm']\n",
    "\n",
    "args = {'node_feature_names': node_features, 'edge_feature_names':edge_feature_names}\n",
    "# DATASETS\n",
    "my_data_train  = CustomDataset('data_train_toy/',\n",
    "                               '/home/sneha/toy_lobe_cleaned_normalised_w_labels_train.csv',\n",
    "                               \"/home/sneha/toy_lobe_binary_labels_train.csv\",\n",
    "                               args = args,\n",
    "                               label_col_name=label_col_name\n",
    "                              )\n",
    "\n",
    "my_data_test  = CustomDataset('data_test_toy/', \n",
    "                              '/home/sneha/toy_lobe_cleaned_normalised_w_labels_test.csv',\n",
    "                              \"/home/sneha/toy_lobe_binary_labels_test.csv\",\n",
    "                              args = args,\n",
    "                              label_col_name=label_col_name\n",
    "                             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f36f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(my_data_train.y)\n",
    "# sampler_train = ImbalancedSampler(my_data_train, num_samples=-1)\n",
    "\n",
    "train_loader = DataLoader(my_data_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# from torch_geometric.loader import NeighborLoader, ImbalancedSampler\n",
    "\n",
    "# train_loader = NeighborLoader(my_data_train,num_neighbors= -1, sampler=sampler_train, batch_size=batch_size, shuffle=False)\n",
    "# # loader = NeighborLoader(data, input_nodes=data.train_mask,\n",
    "# #                         batch_size=64, num_neighbors=[-1, -1],\n",
    "# #                         sampler=sampler, ...)\n",
    "\n",
    "\n",
    "test_loader =  DataLoader(my_data_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ec809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGCN(torch.nn.Module):\n",
    "    '''\n",
    "    Simple GNN for graph preds\n",
    "    hidden_channels = [8,8,8] for now\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, in_channels, hidden_channels = 8, out_channels= 1, dropout=0.2):\n",
    "        super(MyGCN, self).__init__()\n",
    "        self.dropout=dropout\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels,hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels,hidden_channels)\n",
    "        self.lin =  nn.Sequential(\n",
    "                                        nn.Linear(hidden_channels, out_channels),\n",
    "                                        nn.Sigmoid()\n",
    "                                    )\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index,  batch = data.x, data.edge_index, data.batch\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x, None\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# class GlobalAttentionNet(torch.nn.Module):\n",
    "#     '''\n",
    "#     Does per node predictions\n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     def __init__(self, in_channels, out_channels, num_layers, hidden):\n",
    "        \n",
    "#         super().__init__()\n",
    "#         self.conv1 = SAGEConv(in_channels, hidden)\n",
    "#         self.convs = torch.nn.ModuleList()\n",
    "#         for i in range(num_layers - 1):\n",
    "#             self.convs.append(SAGEConv(hidden, hidden))\n",
    "#         self.att = GlobalAttention(Linear(hidden, 1))\n",
    "#         self.lin1 = Linear(hidden, hidden)\n",
    "#         self.lin2 = Linear(hidden, out_channels)\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         for conv in self.convs:\n",
    "#             x = F.relu(conv(x, edge_index))\n",
    "#         x = self.att(x, batch)\n",
    "#         x = F.relu(self.lin1(x))\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.lin2(x)\n",
    "#         return F.log_softmax(x, dim=-1)\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return self.__class__.__name__\n",
    "\n",
    "model = MyGCN(in_channels=13, hidden_channels=8, out_channels=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced7f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))\n",
    "data = data.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23ec563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d232e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make training function\n",
    "def train_model(model, \n",
    "                optimizer,\n",
    "                train_loader, \n",
    "                device,\n",
    "                num_epochs, \n",
    "                count_epochs=0,\n",
    "                verbose=True, \n",
    "                scheduler=None, \n",
    "                accum_iter=1, \n",
    "                class_weight=None):\n",
    "    \n",
    "    plotting_dict_train = {\"loss\":[], \"accuracy\": []}\n",
    "    noisy_label=False\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        denominator_loss = 0\n",
    "        denominator_acc=0\n",
    "        for batch_idx, data in list(enumerate(train_loader)):\n",
    "            data = data.to(device)\n",
    "            batch_size = data.num_graphs\n",
    "            batch_vector = data.batch\n",
    "            denominator_loss+=batch_size\n",
    "            # reset gradients\n",
    "            if batch_idx % accum_iter ==0:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # conduct a forward pass\n",
    "            out, weight = model(data)\n",
    "#             print(\"out\", out.shape, out)\n",
    "            y_per_graph = data.y.float()\n",
    "            \n",
    "              # Noisy vs per graph labelling depending on model type\n",
    "            if (out.squeeze()).shape != data.y.shape:\n",
    "                noisy_label = True\n",
    "                y = torch.take(data.y.float(), batch_vector) # repeat label shape\n",
    "#                 print('Noisy labelling', y.shape)\n",
    "                # denominator for accuracy etc. is y.shape from here (to average out per node accuracy)\n",
    "            else:\n",
    "                y = data.y.float()\n",
    "                print('Not changing label format', y.shape, 'balance', y.sum()/len(y))\n",
    "                # denominatro here is y.shape (per graph to avg out graph accuracy)\n",
    "#             print(y.shape[0])\n",
    "            denominator_acc +=y.shape[0]\n",
    "            # calculate loss and metrics\n",
    "            pred = out > 0.5\n",
    "            pred = pred.long()\n",
    "#             print(pred.unique())\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "            \n",
    "            \n",
    "            if class_weight is not None:\n",
    "                weights = torch.take(class_weight.to(device), y.long()) # class_weight has weight for 0 in posnt 0 and 1 in 1\n",
    "#                 print(\"weights\", weights, y, sep='\\n')\n",
    "                loss = F.binary_cross_entropy(out.squeeze(),y.float(), weight=weights.float())\n",
    "            else:\n",
    "                loss = F.binary_cross_entropy(out.squeeze(),y)\n",
    "                \n",
    "#             print(\"loss\", loss)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # backward pass, normalising for gradient accumulation\n",
    "            loss = loss / accum_iter\n",
    "            loss.backward()\n",
    "            \n",
    "            # step\n",
    "            if (batch_idx % accum_iter == 0) or (batch_idx + 1 == len(train_loader)):\n",
    "                optimizer.step()\n",
    "\n",
    "                \n",
    "            if verbose:\n",
    "                print('Epoch: {}, Batch: {}, Loss: {:.2f}'.format(epoch+count_epochs, batch_idx, loss.item()))\n",
    "        # calculate loss and error for epoch\n",
    "        train_loss /= denominator_loss # loss is already mean over the nodes so just divide by num graphs in batch\n",
    "        accuracy = correct / denominator_acc # if doing noisy labelling need to do length of vector\n",
    "        plotting_dict_train[\"loss\"].append(train_loss)\n",
    "        plotting_dict_train[\"accuracy\"].append(accuracy)\n",
    "        \n",
    "        # step at the end of each epoch\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "            if scheduler is not None:\n",
    "                last_lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "        print('Epoch: {}, Train Loss: {:.5f}, Train Accuracy: {:.5f}'.format(epoch+count_epochs, train_loss, accuracy))\n",
    "        \n",
    "    return plotting_dict_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d40a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if weighted_loss:\n",
    "\n",
    "    class_prop = my_data_train.class_proportions\n",
    "    class_weight_tensor = torch.tensor([1/class_prop[0], 1/class_prop[1]], dtype=float)\n",
    "    mag = class_weight_tensor.sum()\n",
    "    class_weight_tensor = class_weight_tensor/mag\n",
    "    print(\"class weights for weighted loss\", class_weight_tensor)\n",
    "\n",
    "else:\n",
    "    class_weight_tensor = None\n",
    "\n",
    "\n",
    "input_dim_node = my_data_train[0].x.shape[1]\n",
    "print(input_dim_node)\n",
    "model = GAT(input_dim_node = input_dim_node,\n",
    "    agg=model_agg,\n",
    "            dropout=dropout,\n",
    "            hidden_dim1=hidden_dim1,\n",
    "            hidden_dim2=hidden_dim2,\n",
    "               heads1=8,\n",
    "             heads2 = 8,\n",
    "             num_classes=1).to(device)\n",
    "print(device)\n",
    "\n",
    "# model = MyGCN(in_channels=13, hidden_channels=8, out_channels=1)\n",
    "print(model)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "model.train()\n",
    "test_accuracy = []\n",
    "test_f1 = []\n",
    "test_cm = []\n",
    "test_epoch = []\n",
    "train_accuracy = []\n",
    "train_loss = []\n",
    "while count_epochs <= (num_epochs-test_every):\n",
    "    print(f\"Training epochs: {count_epochs} to {count_epochs+test_every} \")\n",
    "    plotting_dict_train = train_model(model, optimizer ,train_loader, device,num_epochs=test_every, count_epochs=count_epochs, verbose=verbose, class_weight =class_weight_tensor)\n",
    "    count_epochs += test_every\n",
    "    train_accuracy.extend(plotting_dict_train['accuracy'])\n",
    "    train_loss.extend(plotting_dict_train['loss']) \n",
    "    \n",
    "    # test model\n",
    "    cm, f1, accuracy =  test_model(model, test_loader, device, count_epochs, threshold=0.5)\n",
    "    test_epoch.append(count_epochs)\n",
    "    test_cm.append(cm)\n",
    "    test_f1.append(f1)\n",
    "    test_accuracy.append(accuracy)\n",
    "    \n",
    "    print(\"Visualise training progress\")\n",
    "    f, (ax1, ax2) = plt.subplots(1,2)\n",
    "    ax1.plot(np.arange(0,len(train_accuracy)), train_accuracy, label='train_accuracy')\n",
    "    ax2.plot(np.arange(0,len(train_loss)), train_loss, label='train_loss')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('accuracy')\n",
    "    ax2.set_ylabel('loss')\n",
    "    plt.title('Training')\n",
    "    plt.savefig(f'/home/sneha/airway_exploration/graph_networks/run_{run_id}/training_epoch_{count_epochs}')\n",
    "    plt.show()\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1,2)\n",
    "    ax1.plot(test_epoch, test_accuracy, label='test_accuracy')\n",
    "    ax2.plot(test_epoch, test_f1, label='test_f1')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('accuracy')\n",
    "    ax2.set_ylabel('f1')\n",
    "    plt.title('Testing')\n",
    "    plt.savefig(f'/home/sneha/airway_exploration/graph_networks/run_{run_id}/test_epoch_{count_epochs}')\n",
    "    plt.show()\n",
    "    \n",
    "# #   vis_ids are one anomalous and one normal all in train data not test\n",
    "#     if model_agg == 'attn':\n",
    "#         for i in range(0,2):\n",
    "\n",
    "#             id_test = label_df.iloc[np.argmax((label_df.binaryLL_1==i))].idno\n",
    "#             index_test = [index for index, (idno, relabel) in train_loader.dataset.node_map.items() if idno==id_test][0]\n",
    "#             _vis_graph_example_2(train_loader,model, index_test, orig_df, device,visualise_g = False, save_path = f'/home/sneha/airway_exploration/graph_networks/run_{run_id}/training_vis_epoch_{count_epochs}_{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad907219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01056bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_vis_graph_example_2(train_loader,model, 100, orig_df, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
