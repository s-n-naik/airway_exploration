{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5d18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "# !apt-get install -y xvfb\n",
    "import time\n",
    "import torch\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader, ImbalancedSampler\n",
    "from torch_geometric.data import Dataset\n",
    "# https://www.youtube.com/watch?v=QLIkOtKS4os --> creating custom dataset in pytorch geometric\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "import torch_geometric\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, GlobalAttention, SAGEConv\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "import seaborn as sn\n",
    "import random\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "from graph_utils import set_device_and_seed, set_seed, show, visualize_graph, visualize_embedding, _count_parameters, visualise_airway_tree_matplotlib\n",
    "from graph_datasets import CustomDataset\n",
    "from graph_models import CustomGlobalAttention, GAT\n",
    "from graph_training import train_model, test_model, _vis_graph_example, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94cead01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    smooth = 1.\n",
    "\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (iflat.sum() + tflat.sum() + smooth))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d478ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomDatasetNode(torch_geometric.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 filename_data,\n",
    "                 filename_labels,\n",
    "                 test=False,\n",
    "                 transform=None,\n",
    "                 pre_transform=None,\n",
    "                 label_col_name = 'binaryLL_1',\n",
    "                 node_level = True,\n",
    "                args = {'node_feature_names': [], 'edge_feature_names': []}):\n",
    "        '''\n",
    "        For NODE LEVEL CLASSIFICATION\n",
    "        root = where dataset should be stored, folder is split into raw_dir and processed_dir\n",
    "        filename_data = contains X features for nodes + edges (csv)\n",
    "        filename_labels= contains Y labels for graphs (csv)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "\n",
    "            \n",
    "            \n",
    "        self.test = False\n",
    "        self.filename_data = os.path.abspath(filename_data)\n",
    "        self.filename_labels = os.path.abspath(filename_labels)\n",
    "        self.node_map = {}\n",
    "        self.node_level = node_level\n",
    "        self.y = None\n",
    "        #         super(CustomDataset, self).__init__(root, transform, pre_transform)\n",
    "        if len(args['node_feature_names']) > 0:\n",
    "            self.node_feature_names = args['node_feature_names']\n",
    "        else:\n",
    "            self.node_feature_names = None\n",
    "        \n",
    "        if len(args['edge_feature_names']) > 0:\n",
    "            self.edge_feature_names = args['edge_feature_names']\n",
    "        else:\n",
    "            self.edge_feature_names = None\n",
    "        print(f\"Using Node features: {self.node_feature_names}, Edge features: {self.edge_feature_names}\")\n",
    "        self.label_col = label_col_name\n",
    "        print(f\"Getting labels from: {self.label_col}\")\n",
    "        super(CustomDatasetNode, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return self.filename_data\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\" If these files are found in raw_dir, processing is skipped: NOTE NOT SURE WHAT THIS IS\"\"\"\n",
    "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "\n",
    "        if self.test:\n",
    "            return [f'data_test_{i}.pt' for i in list(self.data.index)]\n",
    "        else:\n",
    "            return [f'data_{i}.pt' for i in list(self.data.index)]\n",
    "    \n",
    "    def _download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(os.path.abspath(self.raw_paths[0]))\n",
    "        label_df = self._process_labels()\n",
    "        graph_ids = self.data.idno.unique()\n",
    "        \n",
    "        for i, idno in tqdm(list(enumerate(graph_ids))):\n",
    "            # iterate through each patricipant (resetindex is important for node relabelling)\n",
    "            df = self.data.loc[self.data.idno == idno].copy().reset_index()\n",
    "            # build a graph out of the df with node features, edge_features and edge_adjacency\n",
    "            x = self._get_node_features(df)\n",
    "            edge_adjacency = self._get_edge_adjacency(df,index=i)\n",
    "            edge_features = self._get_edge_features(df)\n",
    "            y = self._get_label(idno, label_df, x)\n",
    "            data = Data(x=x,\n",
    "                        edge_index=edge_adjacency,\n",
    "                        edge_attr = edge_features,\n",
    "                        y = y\n",
    "                       )\n",
    "            if self.test:\n",
    "                torch.save(data, \n",
    "                            os.path.join(self.processed_dir, \n",
    "                                         f'data_test_{i}.pt'))\n",
    "            else:\n",
    "                 torch.save(data, \n",
    "                            os.path.join(self.processed_dir, \n",
    "                                         f'data_{i}.pt'))\n",
    "    \n",
    "    \n",
    "    def _process_labels(self):\n",
    "        '''\n",
    "        Reads label df - checks its binary 0,1 labels (REQUIRES COLUMN self.label_col) if not returns an error \n",
    "        Keeps only rows that match idnos in self.data_df which is processed first\n",
    "        measures class proportions and saves to self.class_proportions\n",
    "        returns a df which has a label per idno with label in col called 'y'\n",
    "        '''\n",
    "        label_df = pd.read_csv(os.path.abspath(self.filename_labels))\n",
    "        # binarise \n",
    "        assert self.label_col in label_df.columns, f\"The column {self.label_col} cannot be found\"\n",
    "        # drop unnecessary cols\n",
    "        \n",
    "        # drop rows not matching to data ids\n",
    "        data_df =  pd.read_csv(os.path.abspath(self.filename_data))\n",
    "        label_df_small = label_df.loc[label_df.idno.isin(data_df.idno.unique())]\n",
    "        self.y = label_df_small\n",
    "        print(\"# Graphs\", len(label_df_small), \"Label Frequency\", Counter(label_df_small[self.label_col].to_list()))\n",
    "        self.class_proportions = {k:v/len(label_df_small) for k,v in Counter(label_df_small[self.label_col].to_list()).items()}\n",
    "        print(f\"Class proportions: {self.class_proportions}\")\n",
    "        return label_df_small\n",
    "    \n",
    "    def _get_edge_adjacency(self,df, index):\n",
    "        '''\n",
    "        NOTE pytorch requires nodes to start from 0 and go up in integers so need to remap start and end bpids from df\n",
    "        Turns endbpid 1 > 1, if the next one in the df is endbpid=6 it becomes 2 etc so that the endbpids are in order from 1 to max (index+1)\n",
    "        Adds trachea (node 0)\n",
    "        applyys relabelling to start and endbpid based on the dict \n",
    "        returns torch tensor in COO format which is a parallel list [[source_node_list], [corresponding_end_node_list]]\n",
    "        in this format, at list[0][2] and list[1][2] will be source-end node of the second edge in the list\n",
    "        '''\n",
    "        # reindex start / endbpids\n",
    "        \n",
    "        relabel_map = {v:k+1 for k,v in df.endbpid.to_dict().items()}\n",
    "        # add trachea map (node 0)\n",
    "        relabel_map[-1] = 0\n",
    "        # save mapping\n",
    "        self.node_map[index] = (df.idno.unique().item(), relabel_map)\n",
    "        # apply relabelling to source and end nodes\n",
    "        source_nodes = df.startbpid.apply(lambda x: relabel_map[x]).to_list()\n",
    "        end_nodes = df.endbpid.apply(lambda x: relabel_map[x]).to_list()\n",
    "        # return in COO format\n",
    "        return torch.tensor([source_nodes, end_nodes], dtype=torch.long)\n",
    "\n",
    "    def _get_node_features(self,df):\n",
    "        '''\n",
    "        DF already contains normalised features\n",
    "        Feature names in node_features\n",
    "        Format to return is a tensor of shape num_nodes x node_feature_dim with dtype float \n",
    "        assumes ordered in ascending order with nodes 0 and up in integers (so node = index / row num)\n",
    "            node_features = ['x_norm', 'y_norm', 'z_norm', 'dircosx_norm',\n",
    "       'dircosy_norm', 'dircosz_norm','lobe_norm',\n",
    "       'sublobe_norm','angle_norm', 'weibel_generation_norm','dist_nn_in_lobe_norm', 'num_desc_norm','max_path_length_norm']\n",
    "\n",
    "        '''\n",
    "        # nodes and features (pos, direction,lobe + sublobe categorical, angle to parent, weibel gen, dist to nearest neighbor in lobe, max path length to leaf, num descendents of node)\n",
    "        node_features = self.node_feature_names\n",
    "\n",
    "        # adding trachea info to top of list\n",
    "        trachea_dict = dict.fromkeys(node_features, 0)\n",
    "        for i in ['nx', 'ny', 'nz']:\n",
    "            trachea_dict[i] = df.loc[df.endbpid==1][str('parent_loc_'+i)].item()\n",
    "        \n",
    "        # currently usnig as features \n",
    "        list_of_nodes = df[node_features].to_dict(orient='records')\n",
    "        # add trachea to nodes\n",
    "        list_of_lists_nodes =[list(trachea_dict.values())]+ [list(node_feature.values()) for node_feature in list_of_nodes]\n",
    "        x = torch.tensor(list_of_lists_nodes, dtype=torch.float)\n",
    "#         print(\"Shape node features\", x.shape)\n",
    "        return x\n",
    "\n",
    "    def _get_edge_features(self,df):\n",
    "        '''\n",
    "        get matrix of shape [# edges, edge feature size] with type float\n",
    "        '''\n",
    "        if self.edge_feature_names is not None:\n",
    "            edge_feature_names = self.edge_feature_names\n",
    "        else:\n",
    "            edge_feature_names = ['centerlinelength_norm','avginnerarea_norm']\n",
    "            \n",
    "        edge_norm = df[edge_feature_names].values\n",
    "        return torch.tensor(edge_norm, dtype=torch.float)\n",
    "\n",
    "    def _get_label(self, idno, label_df,node_features):\n",
    "        '''\n",
    "        for the selected idno, returns value in self.label_col as an integer\n",
    "        '''\n",
    "        # Assume trachea node is always non anomalous (0)\n",
    "        if self.node_level:\n",
    "            labels = label_df.loc[label_df.idno==idno,\n",
    "                 self.label_col].values\n",
    "            \n",
    "            num_nodes = node_features.shape[0]\n",
    "            if num_nodes -1 == labels.shape[0]:\n",
    "#                 print('adding in label for node -1 = 0 in pytorch method')\n",
    "                labels = [0] + list(labels)\n",
    "            \n",
    "#                 print('Num nodes, num labels', num_nodes, len(labels))\n",
    "            \n",
    "            return torch.tensor(labels, dtype=torch.int64)\n",
    "        else:\n",
    "            # return single graph label\n",
    "            labels = label_df.loc[label_df.idno==idno,\n",
    "                 self.label_col].unique()\n",
    "#             print('num labels per graph', labels.shape[0])\n",
    "        \n",
    "            return torch.tensor(labels, dtype=torch.int64)\n",
    "    \n",
    "    def len(self):\n",
    "        return int(self.data.idno.nunique())\n",
    "    \n",
    "    def get(self, idx):\n",
    "        '''\n",
    "        Equivalent to __getitem__ in pytorch\n",
    "        '''\n",
    "        if self.test:\n",
    "            data = torch.load(os.path.join(self.processed_dir, \n",
    "                                 f'data_test_{idx}.pt'))\n",
    "        else:\n",
    "            data = torch.load(os.path.join(self.processed_dir, \n",
    "                                 f'data_{idx}.pt'))   \n",
    "        return data\n",
    "            \n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e600ac78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idno</th>\n",
       "      <th>startbpid</th>\n",
       "      <th>endbpid</th>\n",
       "      <th>graph_label</th>\n",
       "      <th>node_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.209760e+05</td>\n",
       "      <td>120976.000000</td>\n",
       "      <td>120976.000000</td>\n",
       "      <td>120976.000000</td>\n",
       "      <td>120976.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.474256e+06</td>\n",
       "      <td>49.485708</td>\n",
       "      <td>75.583455</td>\n",
       "      <td>0.570989</td>\n",
       "      <td>0.142045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.769592e+06</td>\n",
       "      <td>63.740793</td>\n",
       "      <td>79.533403</td>\n",
       "      <td>0.494937</td>\n",
       "      <td>0.349097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.010007e+06</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.014340e+06</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.022045e+06</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.016549e+06</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.024979e+06</td>\n",
       "      <td>655.000000</td>\n",
       "      <td>744.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               idno      startbpid        endbpid    graph_label  \\\n",
       "count  1.209760e+05  120976.000000  120976.000000  120976.000000   \n",
       "mean   5.474256e+06      49.485708      75.583455       0.570989   \n",
       "std    1.769592e+06      63.740793      79.533403       0.494937   \n",
       "min    3.010007e+06      -1.000000       1.000000       0.000000   \n",
       "25%    4.014340e+06      10.000000      20.000000       0.000000   \n",
       "50%    5.022045e+06      25.000000      48.000000       1.000000   \n",
       "75%    7.016549e+06      62.000000     104.000000       1.000000   \n",
       "max    8.024979e+06     655.000000     744.000000       1.000000   \n",
       "\n",
       "          node_label  \n",
       "count  120976.000000  \n",
       "mean        0.142045  \n",
       "std         0.349097  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_df = pd.read_csv(os.path.abspath('/home/sneha/toy_normalised_1407.csv'))\n",
    "orig_df.describe()\n",
    "\n",
    "label_df = pd.read_csv(os.path.abspath('/home/sneha/toy_labels_1407.csv'))\n",
    "label_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2efab662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Label frequency distribution [(0, 103792), (1, 17184)]\n",
      "Getting train test split stratified on the 120976 labels\n",
      "Overall Label frequency distribution [(0, 103792), (1, 17184)]\n"
     ]
    }
   ],
   "source": [
    "label_col_name = 'node_label'\n",
    "train_ids, test_ids = train_test_split(label_df, n_splits_test = 5, label_col_name= label_col_name, seed=0)\n",
    "\n",
    "pilot_df_train = orig_df.loc[orig_df.idno.isin(train_ids)]\n",
    "pilot_df_test = orig_df.loc[orig_df.idno.isin(test_ids)]\n",
    "binary_label_df_train = label_df.loc[label_df.idno.isin(train_ids)]\n",
    "binary_label_df_test = label_df.loc[label_df.idno.isin(test_ids)]\n",
    "print(\"Overall Label frequency distribution\", [(x, binary_label_df_test[label_col_name].tolist().count(x)) for x in set(binary_label_df_test[label_col_name].tolist())])\n",
    "\n",
    "\n",
    "# SAVE PILOT DF for training\n",
    "pilot_df_train.to_csv('/home/sneha/toy_lobe_cleaned_normalised_w_labels_train.csv')\n",
    "binary_label_df_train.to_csv('/home/sneha/toy_lobe_binary_labels_train.csv')\n",
    "pilot_df_test.to_csv('/home/sneha/toy_lobe_cleaned_normalised_w_labels_test.csv')\n",
    "binary_label_df_test.to_csv('/home/sneha/toy_lobe_binary_labels_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50fbfdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Node features: ['nx', 'ny', 'nz', 'dircosx_norm', 'dircosy_norm', 'dircosz_norm', 'angle_norm', 'weibel_generation_norm', 'dist_nn_in_lobe_norm', 'num_desc_norm', 'max_path_length_norm', 'centerlinelength_norm', 'avginnerarea_norm', 'lobe_norm', 'num_children'], Edge features: ['centerlinelength_norm', 'avginnerarea_norm']\n",
      "Getting labels from: node_label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Graphs 120976 Label Frequency Counter({0: 103792, 1: 17184})\n",
      "Class proportions: {0: 0.857955296918397, 1: 0.14204470308160297}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2093/2093 [00:12<00:00, 166.70it/s]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Node features: ['nx', 'ny', 'nz', 'dircosx_norm', 'dircosy_norm', 'dircosz_norm', 'angle_norm', 'weibel_generation_norm', 'dist_nn_in_lobe_norm', 'num_desc_norm', 'max_path_length_norm', 'centerlinelength_norm', 'avginnerarea_norm', 'lobe_norm', 'num_children'], Edge features: ['centerlinelength_norm', 'avginnerarea_norm']\n",
      "Getting labels from: node_label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Graphs 120976 Label Frequency Counter({0: 103792, 1: 17184})\n",
      "Class proportions: {0: 0.857955296918397, 1: 0.14204470308160297}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2093/2093 [00:12<00:00, 168.45it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "label_col_name='node_label'\n",
    "\n",
    "#  removed from features for toy example, added edge features to node ones too just in case\n",
    "node_features = ['nx', 'ny', 'nz', 'dircosx_norm',\n",
    "               'dircosy_norm', 'dircosz_norm','angle_norm', 'weibel_generation_norm','dist_nn_in_lobe_norm',\n",
    "                 'num_desc_norm','max_path_length_norm','centerlinelength_norm','avginnerarea_norm',\n",
    "                'lobe_norm', 'num_children']\n",
    "edge_feature_names = ['centerlinelength_norm','avginnerarea_norm']\n",
    "\n",
    "args = {'node_feature_names': node_features, 'edge_feature_names':edge_feature_names}\n",
    "# DATASETS\n",
    "\n",
    "\n",
    "my_data_train  = CustomDatasetNode('data_train_toy/',\n",
    "                               '/home/sneha/toy_lobe_cleaned_normalised_w_labels_train.csv',\n",
    "                               '/home/sneha/toy_lobe_binary_labels_train.csv',\n",
    "                               args = args,\n",
    "                               label_col_name=label_col_name,\n",
    "                                node_level=True\n",
    "                              )\n",
    "\n",
    "my_data_test  = CustomDatasetNode('data_test_toy/',\n",
    "                               '/home/sneha/toy_lobe_cleaned_normalised_w_labels_test.csv',\n",
    "                               '/home/sneha/toy_lobe_binary_labels_test.csv',\n",
    "                               args = args,\n",
    "                               label_col_name=label_col_name,\n",
    "                                node_level=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd5eb861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,\n",
       " tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "         126, 127]),\n",
       " EdgeIndex(edge_index=tensor([[  0,   1,   1,   3,   3,   2,   2,   4,   4,   5,   5,   6,   6,   7,\n",
       "            7,   9,   9,   8,   8,  10,  10,  11,  11,  13,  14,  14,  15,  15,\n",
       "           16,  16,  18,  18,  20,  20,  21,  21,  24,  24,  26,  26,  29,  29,\n",
       "           31,  31,  37,  37,  41,  41,  49,  50,  50,  51,  51,  52,  52,  54,\n",
       "           54,  53,  53,  56,  56,  55,  55,  57,  58,  58,  60,  60,  61,  62,\n",
       "           62,  64,  64,  65,  65,  67,  67,  68,  68,  70,  70,  72,  73,  73,\n",
       "           73,  78,  78,  80,  80,  82,  82,  83,  83,  84,  84,  85,  85,  87,\n",
       "           87,  89,  89,  90,  90,  90,  91,  91,  92,  92,  94,  94,  86,  86,\n",
       "          110, 110, 111, 111, 117, 117, 116, 116, 122, 123, 123, 125, 125],\n",
       "         [  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "           15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "           29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "           43,  44,  45,  46,  47,  48,  50,  51,  52,  53,  54,  55,  56,  57,\n",
       "           58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,\n",
       "           72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,\n",
       "           86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,\n",
       "          100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113,\n",
       "          114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127]]), e_id=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124]), size=(128, 128)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from torch_geometric.loader import NeighborLoader, ImbalancedSampler, NeighborSampler\n",
    "from torch_geometric.data import Batch\n",
    "train_obj = Batch.from_data_list(my_data_train) # loading all graphs into batch object\n",
    "train_obj.n_id = torch.arange(train_obj.num_nodes)\n",
    "sampler = ImbalancedSampler(train_obj)\n",
    "# train_loader = NeighborLoader(train_obj,\n",
    "#                                num_neighbors=[15, 10, 5], batch_size=128,\n",
    "#                               sampler=sampler,\n",
    "#                                shuffle=False)\n",
    "train_loader = NeighborSampler(train_obj.edge_index,\n",
    "                               sizes=[15, 10, 5], batch_size=128,\n",
    "                              sampler=sampler,\n",
    "                               shuffle=False)\n",
    "\n",
    "test_obj = Batch.from_data_list(my_data_test) # loading all graphs into batch object\n",
    "test_obj.n_id = torch.arange(test_obj.num_nodes)\n",
    "test_loader = NeighborSampler(test_obj.edge_index, node_idx=None, sizes=[-1], batch_size=128, shuffle=False)\n",
    "\n",
    "# print(my_data_train.y)\n",
    "# sampler_train = ImbalancedSampler(my_data_train)\n",
    "\n",
    "\n",
    "# indices = torch.where(train_obj.y==0)[0]#filtering majority class\n",
    "# print(len(indices)/len(train_obj.y), indices)\n",
    "# train_loader = NeighborLoader(train_obj,num_neighbors=[-1]*3, input_nodes=indices, batch_size=128, shuffle=True)  \n",
    "next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7101339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65477308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([504, 15])\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_loader))\n",
    "batch_size, n_id, adjs = data\n",
    "all_data = train_obj.x\n",
    "print(all_data[n_id].shape)\n",
    "print(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fec8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = x.relu_()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, x_all, subgraph_loader):\n",
    "        pbar = tqdm(total=len(subgraph_loader.dataset) * len(self.convs))\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        # Compute representations of nodes layer by layer, using *all*\n",
    "        # available edges. This leads to faster computation in contrast to\n",
    "        # immediately computing the final representations of each batch:\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            xs = []\n",
    "            for batch in subgraph_loader:\n",
    "                x = x_all[batch.n_id.to(x_all.device)].to(device)\n",
    "                x = conv(x, batch.edge_index.to(device))\n",
    "                if i < len(self.convs) - 1:\n",
    "                    x = x.relu_()\n",
    "                xs.append(x[:batch.batch_size].cpu())\n",
    "                pbar.update(batch.batch_size)\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "        pbar.close()\n",
    "        return x_all\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be64a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        self.lin =  nn.Sequential(\n",
    "                                        nn.Linear(hidden_channels, out_channels),\n",
    "                                        nn.Sigmoid()\n",
    "                                    )\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        # `train_loader` computes the k-hop neighborhood of a batch of nodes,\n",
    "        # and returns, for each layer, a bipartite graph object, holding the\n",
    "        # bipartite edges `edge_index`, the index `e_id` of the original edges,\n",
    "        # and the size/shape `size` of the bipartite graph.\n",
    "        # Target nodes are also included in the source nodes so that one can\n",
    "        # easily apply skip-connections or add self-loops.\n",
    "        for i, (edge_index, e_id, size) in enumerate(adjs):\n",
    "            xs = []\n",
    "#             print(i, edge_index, e_id, size, sep='\\n')\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            \n",
    "            if i != self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "            xs.append(x)\n",
    "            if i == 0: \n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_1_embeddings = x_all\n",
    "            elif i == 1:\n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_2_embeddings = x_all\n",
    "            elif i == 2:\n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_3_embeddings = x_all    \n",
    "        #return x.log_softmax(dim=-1)\n",
    "        \n",
    "        x_out = self.lin(layer_3_embeddings)\n",
    "       \n",
    "        return layer_1_embeddings, layer_2_embeddings, layer_3_embeddings, x_out\n",
    "\n",
    "    def inference(self, x_all):\n",
    "        pbar = tqdm(total=x_all.size(0) * self.num_layers)\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        # Compute representations of nodes layer by layer, using *all*\n",
    "        # available edges. This leads to faster computation in contrast to\n",
    "        # immediately computing the final representations of each batch.\n",
    "        total_edges = 0\n",
    "        for i in range(self.num_layers):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                edge_index, _, size = adj.to(device)\n",
    "                total_edges += edge_index.size(1)\n",
    "                x = x_all[n_id].to(device)\n",
    "                x_target = x[:size[1]]\n",
    "                x = self.convs[i]((x, x_target), edge_index)\n",
    "                if i != self.num_layers - 1:\n",
    "                    x = F.relu(x)\n",
    "                xs.append(x)\n",
    "\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "            if i == 0: \n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_1_embeddings = x_all\n",
    "            elif i == 1:\n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_2_embeddings = x_all\n",
    "            elif i == 2:\n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_3_embeddings = x_all\n",
    "                \n",
    "        pbar.close()\n",
    "\n",
    "        return layer_1_embeddings, layer_2_embeddings, layer_3_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3524be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n",
      "Setting torch, cuda, numpy and random seeds to 0\n",
      "SAGE(\n",
      "  (convs): ModuleList(\n",
      "    (0): SAGEConv(15, 8)\n",
      "    (1): SAGEConv(8, 8)\n",
      "    (2): SAGEConv(8, 8)\n",
      "  )\n",
      "  (lin): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "torch.Size([123069, 15]) torch.Size([123069])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Epoch 01:   0%|                                                                                 | 0/962 [00:05<?, ?it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Loss: 0.0014, Approx. Train: 0.9341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Epoch 02:   0%|                                                                                 | 0/962 [00:05<?, ?it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02, Loss: 0.0011, Approx. Train: 0.9477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Epoch 03:   0%|                                                                                 | 0/962 [00:05<?, ?it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03, Loss: 0.0011, Approx. Train: 0.9505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Epoch 04:   0%|                                                                                 | 0/962 [00:05<?, ?it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04, Loss: 0.0010, Approx. Train: 0.9518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Epoch 05:   0%|                                                                                 | 0/962 [00:05<?, ?it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05, Loss: 0.0011, Approx. Train: 0.9549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Epoch 06:   0%|                                                                                 | 0/962 [00:05<?, ?it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06, Loss: 0.0010, Approx. Train: 0.9547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Epoch 07:   0%|                                                                                 | 0/962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# def train(epoch):\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#     https://github.com/pyg-team/pytorch_geometric/blob/master/examples/reddit.py\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#     model.train()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#     return total_loss / total_examples, total_correct / total_examples\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m---> 82\u001b[0m     loss, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Approx. Train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     36\u001b[0m         loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(x_out\u001b[38;5;241m.\u001b[39msqueeze(),labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#         print('loss',loss)\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/my_env/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/my_env/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = set_device_and_seed(GPU=True, gpu_name='cuda:0')\n",
    "in_features = my_data_train[0].x.shape[1]\n",
    "out_features= 1\n",
    "model = SAGE(in_features, 8, out_features)\n",
    "print(model)\n",
    "model = model.to(device)\n",
    "x = train_obj.x.to(device)\n",
    "y = train_obj.y.squeeze().to(device)\n",
    "print(x.shape, y.shape)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train(epoch):\n",
    "    # https://towardsdatascience.com/a-comprehensive-case-study-of-graphsage-algorithm-with-hands-on-experience-using-pytorchgeometric-6fc631ab1067\n",
    "    model.train()\n",
    "    pbar = tqdm(total=int(len(train_loader)))\n",
    "    pbar.set_description(f'Epoch {epoch:02d}')\n",
    "    #pbar = tqdm(total=train_idx.size(0))\n",
    "    #pbar.set_description(f'Epoch {epoch:02d}')\n",
    "    denom = 0\n",
    "    total_loss = total_correct = 0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        denom += batch_size\n",
    "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        optimizer.zero_grad()    \n",
    "        l1_emb, l2_emb, l3_emb, x_out = model(x[n_id], adjs)\n",
    "#         print(\"Layer 1 embeddings\", l1_emb.shape)\n",
    "#         print(\"Layer 2 embeddings\", l2_emb.shape)\n",
    "#         print(\"Layer 3 embeddings\", l3_emb.shape, l3_emb)\n",
    "#         print('xout', x_out.shape, x_out)\n",
    "#         out = l3_emb.log_softmax(dim=-1)\n",
    "#         print(out)\n",
    "        labels = y[n_id[:batch_size]]\n",
    "#         print('labels', labels.shape, labels)\n",
    "#         loss = F.nll_loss(x_out, y[n_id[:batch_size]])\n",
    "        loss = F.binary_cross_entropy(x_out.squeeze(),labels.float())\n",
    "#         print('loss',loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss.item())\n",
    "        pred = x_out > 0.5\n",
    "        pred = pred.long()\n",
    "#             print(pred.unique())\n",
    "        total_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "#         total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\n",
    "        #pbar.update(batch_size)\n",
    "\n",
    "    #pbar.close()\n",
    "\n",
    "    loss = total_loss / denom\n",
    "    approx_acc = total_correct / denom\n",
    "\n",
    "    return loss, approx_acc\n",
    "\n",
    "# def train(epoch):\n",
    "#     https://github.com/pyg-team/pytorch_geometric/blob/master/examples/reddit.py\n",
    "#     model.train()\n",
    "\n",
    "#     pbar = tqdm(total=int(len(train_loader.dataset)))\n",
    "#     pbar.set_description(f'Epoch {epoch:02d}')\n",
    "\n",
    "#     total_loss = total_correct = total_examples = 0\n",
    "#     for batch in train_loader:\n",
    "#         batch = batch.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         y = batch.y[:batch.batch_size]\n",
    "#         y_hat = model(batch.x, batch.edge_index)[:batch.batch_size]\n",
    "#         print('preds', y_hat, 'orig', y)\n",
    "#         loss = F.cross_entropy(y_hat, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += float(loss) * batch.batch_size\n",
    "#         total_correct += int((y_hat.argmax(dim=-1) == y).sum())\n",
    "#         total_examples += batch.batch_size\n",
    "#         pbar.update(batch.batch_size)\n",
    "#     pbar.close()\n",
    "\n",
    "#     return total_loss / total_examples, total_correct / total_examples\n",
    "losses = []\n",
    "for epoch in range(1, 11):\n",
    "    loss, acc = train(epoch)\n",
    "    print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1142b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_loader)).to(device)\n",
    "# for data in train_loader:\n",
    "print(data.edge_index[:data.batch_size])\n",
    "y = data.y[:data.batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SAGE(dataset.num_features, 256, dataset.num_classes).to(device)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
